{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f2d863b2",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'genfromtxt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 163\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dataset \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtitanic\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    161\u001b[0m     \u001b[38;5;66;03m# Load titanic data\u001b[39;00m\n\u001b[1;32m    162\u001b[0m     path_train \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./dataset/titanic/titanic_training.csv\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m--> 163\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[43mgenfromtxt\u001b[49m(path_train, delimiter\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m'\u001b[39m, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    164\u001b[0m     path_test \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./dataset/titanic/titanic_test_data.csv\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    165\u001b[0m     test_data \u001b[38;5;241m=\u001b[39m genfromtxt(path_test, delimiter\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m'\u001b[39m, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'genfromtxt' is not defined"
     ]
    }
   ],
   "source": [
    "class DecisionTree:\n",
    "    def __init__(self, max_depth=3, feature_labels=None):\n",
    "        self.max_depth = max_depth\n",
    "        self.features = feature_labels\n",
    "        self.left, self.right = None, None  # for non-leaf nodes\n",
    "        self.split_idx, self.thresh = None, None  # for non-leaf nodes\n",
    "        self.data, self.pred = None, None  # for leaf nodes\n",
    "        \n",
    "    def accuracy(self, X, y):\n",
    "        yhat = self.predict(X)\n",
    "        return np.sum(yhat == y) / float(len(y))\n",
    "\n",
    "    @staticmethod\n",
    "    def information_gain(X, y, thresh):\n",
    "        \n",
    "        # Split the samples using the threshold value\n",
    "        idx_left = X < thresh\n",
    "        idx_right = X >= thresh\n",
    "        y_left = y[idx_left]\n",
    "        y_right = y[idx_right]\n",
    "        \n",
    "        # Calculate the Gini impurity of each split\n",
    "        impurity_left = DecisionTree.gini_impurity(X, y_left, thresh)\n",
    "        impurity_right = DecisionTree.gini_impurity(X, y_right, thresh)\n",
    "        \n",
    "        # Calculate the weighted average of the Gini impurities\n",
    "        n_total = len(y)\n",
    "        n_left = len(y_left)\n",
    "        n_right = len(y_right)\n",
    "        impurity_avg = (n_left / n_total) * impurity_left + (n_right / n_total) * impurity_right\n",
    "        \n",
    "        # Calculate the Gini impurity of the parent node\n",
    "        impurity_parent = DecisionTree.gini_impurity(X, y, thresh)\n",
    "        \n",
    "        # Calculate the information gain\n",
    "        information_gain = impurity_parent - impurity_avg\n",
    "        \n",
    "        return information_gain\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def gini_impurity(X, y, thresh):\n",
    "        \n",
    "        # Count the frequency of each class label\n",
    "        counts = np.bincount(y)\n",
    "        \n",
    "        # Calculate the probability of each class label\n",
    "        probs = counts / len(y)\n",
    "        \n",
    "        # Calculate the Gini impurity\n",
    "        impurity = 1 - np.sum(probs ** 2)\n",
    "        \n",
    "        return impurity\n",
    "\n",
    "\n",
    "\n",
    "    def split(self, X, y, idx, thresh):\n",
    "        X0, idx0, X1, idx1 = self.split_test(X, idx=idx, thresh=thresh)\n",
    "        y0, y1 = y[idx0], y[idx1]\n",
    "        return X0, y0, X1, y1\n",
    "\n",
    "    def split_test(self, X, idx, thresh):\n",
    "        idx0 = np.where(X[:, idx] < thresh)[0]\n",
    "        idx1 = np.where(X[:, idx] >= thresh)[0]\n",
    "        X0, X1 = X[idx0, :], X[idx1, :]\n",
    "        return X0, idx0, X1, idx1\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        if self.max_depth > 0:\n",
    "            # compute entropy gain for all single-dimension splits,\n",
    "            # thresholding with a linear interpolation of 10 values\n",
    "            gains = []\n",
    "            # The following logic prevents thresholding on exactly the minimum\n",
    "            # or maximum values, which may not lead to any meaningful node\n",
    "            # splits.\n",
    "            thresh = np.array([\n",
    "                np.linspace(np.min(X[:, i]) + eps, np.max(X[:, i]) - eps, num=10)\n",
    "                for i in range(X.shape[1])\n",
    "            ])\n",
    "            for i in range(X.shape[1]):\n",
    "                gains.append([self.information_gain(X[:, i], y, t) for t in thresh[i, :]])\n",
    "\n",
    "            gains = np.nan_to_num(np.array(gains))\n",
    "            self.split_idx, thresh_idx = np.unravel_index(np.argmax(gains), gains.shape)\n",
    "            self.thresh = thresh[self.split_idx, thresh_idx]\n",
    "            X0, y0, X1, y1 = self.split(X, y, idx=self.split_idx, thresh=self.thresh)\n",
    "            if X0.size > 0 and X1.size > 0:\n",
    "                self.left = DecisionTree(\n",
    "                    max_depth=self.max_depth - 1, feature_labels=self.features)\n",
    "                self.left.fit(X0, y0)\n",
    "                self.right = DecisionTree(\n",
    "                    max_depth=self.max_depth - 1, feature_labels=self.features)\n",
    "                self.right.fit(X1, y1)\n",
    "            else:\n",
    "                self.max_depth = 0\n",
    "                self.data, self.labels = X, y\n",
    "                self.pred = stats.mode(y).mode[0]\n",
    "        else:\n",
    "            self.data, self.labels = X, y\n",
    "            self.pred = stats.mode(y).mode[0]\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        if self.max_depth == 0:\n",
    "            return self.pred * np.ones(X.shape[0])\n",
    "        else:\n",
    "            X0, idx0, X1, idx1 = self.split_test(X, idx=self.split_idx, thresh=self.thresh)\n",
    "            yhat = np.zeros(X.shape[0])\n",
    "            yhat[idx0] = self.left.predict(X0)\n",
    "            yhat[idx1] = self.right.predict(X1)\n",
    "            return yhat\n",
    "\n",
    "    def __repr__(self):\n",
    "        if self.max_depth == 0:\n",
    "            return \"%s (%s)\" % (self.pred, self.labels.size)\n",
    "        else:\n",
    "            return \"[%s < %s: %s | %s]\" % (self.features[self.split_idx],\n",
    "                                           self.thresh, self.left.__repr__(),\n",
    "                                           self.right.__repr__())\n",
    "        \n",
    "def preprocess(data, fill_mode=True, min_freq=10, onehot_cols=[]):\n",
    "    # fill_mode = False\n",
    "\n",
    "    # Temporarily assign -1 to missing data\n",
    "    data[data == ''] = '-1'\n",
    "\n",
    "    # Hash the columns (used for handling strings)\n",
    "    onehot_encoding = []\n",
    "    onehot_features = []\n",
    "    for col in onehot_cols:\n",
    "        counter = Counter(data[:, col])\n",
    "        for term in counter.most_common():\n",
    "            if term[0] == '-1':\n",
    "                continue\n",
    "            if term[-1] <= min_freq:\n",
    "                break\n",
    "            onehot_features.append(term[0])\n",
    "            onehot_encoding.append((data[:, col] == term[0]).astype(float))\n",
    "        data[:, col] = '0'\n",
    "    onehot_encoding = np.array(onehot_encoding).T\n",
    "    data = np.hstack([np.array(data, dtype=float), np.array(onehot_encoding)])\n",
    "\n",
    "    # Replace missing data with the mode value. We use the mode instead of\n",
    "    # the mean or median because this makes more sense for categorical\n",
    "    # features such as gender or cabin type, which are not ordered.\n",
    "    if fill_mode:\n",
    "        for i in range(data.shape[-1]):\n",
    "            mode = stats.mode(data[((data[:, i] < -1 - eps) +\n",
    "                                    (data[:, i] > -1 + eps))][:, i]).mode[0]\n",
    "            data[(data[:, i] > -1 - eps) * (data[:, i] < -1 + eps)][:, i] = mode\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    dataset = \"titanic\"\n",
    "    params = {\n",
    "        \"max_depth\": 5,\n",
    "        # \"random_state\": 6,\n",
    "        \"min_samples_leaf\": 10,\n",
    "    }\n",
    "    N = 100\n",
    "    if dataset == \"titanic\":\n",
    "        # Load titanic data\n",
    "        path_train = './dataset/titanic/titanic_training.csv'\n",
    "        data = genfromtxt(path_train, delimiter=',', dtype=None, encoding=None)\n",
    "        path_test = './dataset/titanic/titanic_test_data.csv'\n",
    "        test_data = genfromtxt(path_test, delimiter=',', dtype=None, encoding=None)\n",
    "        y = data[1:, -1]  # label = survived\n",
    "        class_names = [\"Died\", \"Survived\"]\n",
    "        labeled_idx = np.where(y != '')[0]\n",
    "\n",
    "        y = np.array(y[labeled_idx])\n",
    "        y = y.astype(float).astype(int)\n",
    "\n",
    "        #cleaning data\n",
    "        data = np.delete(data, 5 ,axis=1)\n",
    "        data = np.delete(data, 7 ,axis=1)\n",
    "        \n",
    "        #cleaning test data\n",
    "        test_data = np.delete(test_data, 5 ,axis=1)\n",
    "        test_data = np.delete(test_data, 7 ,axis=1)\n",
    "        \n",
    "        print(\"\\n\\nPart (b): preprocessing the titanic dataset\")\n",
    "        X, onehot_features = preprocess(data[1:, :-1], onehot_cols=[1,5,6,7])\n",
    "        X = X[labeled_idx, :]\n",
    "        Z, _ = preprocess(test_data[1:, :], onehot_cols=[1,5,6,7])\n",
    "        assert X.shape[1] == Z.shape[1]\n",
    "        features = list(data[0, :-1]) + onehot_features\n",
    "        \n",
    "        \n",
    "\n",
    "    elif dataset == \"spam\":\n",
    "        features = [\n",
    "            \"pain\", \"private\", \"bank\", \"money\", \"drug\", \"spam\", \"prescription\", \"creative\",\n",
    "            \"height\", \"featured\", \"differ\", \"width\", \"other\", \"energy\", \"business\", \"message\",\n",
    "            \"volumes\", \"revision\", \"path\", \"meter\", \"memo\", \"planning\", \"pleased\", \"record\", \"out\",\n",
    "            \"semicolon\", \"dollar\", \"sharp\", \"exclamation\", \"parenthesis\", \"square_bracket\",\n",
    "            \"ampersand\"\n",
    "        ]\n",
    "        assert len(features) == 32\n",
    "\n",
    "        # Load spam data\n",
    "        path_train = './dataset/spam/spam_data.mat'\n",
    "        data = scipy.io.loadmat(path_train)\n",
    "        X = data['training_data']\n",
    "        y = np.squeeze(data['training_labels'])\n",
    "        Z = data['test_data']\n",
    "        class_names = [\"Ham\", \"Spam\"]\n",
    "\n",
    "    else:\n",
    "        raise NotImplementedError(\"Dataset %s not handled\" % dataset)\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
